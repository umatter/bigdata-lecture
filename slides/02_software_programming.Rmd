---
title: "Big Data Analytics"
subtitle: 'Lecture 2: Programming with (Big) Data'
author: "Prof. Dr. Ulrich Matter"
output:
  ioslides_presentation:
    css: ../style/ioslides_unilu.css
    template: ../style/nologo_template.html
  beamer_presentation: default
logo: ../img/logo_unilu2.png
bibliography: ../references/bigdata.bib
---


```{r set-options, echo=FALSE, cache=FALSE}
options(width = 100)
library(knitr)
```


<!-- # Updates -->


<!-- ## Group Examinations -->

<!-- - Make sure to build teams of 4 (3). -->
<!-- - Register your team (and all members): -->
<!-- - *Team name*: important for handing out team assignments via GitHub Classroom. -->

# Platform: Computing Environment

## Computing environment perspective  {data-background=#ffffff}

```{r whatis, echo=FALSE, out.width = "85%", fig.align='center', purl=FALSE}
include_graphics("../img/II_computing_environment.png")
```


## Goals for this session

- Know basic tools to assess performance of an R-program/script.
- Know the most common tricks to write more efficient R code.
- Know the very basics of SQL.
- (Be able to set up an SQLite database on your own machine and import data to it.)



# Software: Programming with (Big) Data

## Programming with (Big) data: common tasks

- Procedures to import/export data.
- Procedures to clean and filter data.
- Implement functions for statistical analysis.

## Programming with (Big) data: considerations

1. Which basic (already implemented) R functions are more or less suitable as building blocks for the program?
2. How can we exploit/avoid some of R's characteristics?
3. Is there a need to interface with a lower-level programming language? (advanced topic)



# Measuring R performance



## Measuring performance

Usual angle: measure how long a script/code chunk needs to run through, find ways to speed it up.

A simple example:

```{r}
# how much time does it take to run this loop?
system.time(for (i in 1:100) {i + 5})
```

## Measuring performance: speed

```{r}
# load package
library(microbenchmark)
# how much time does it take to run this loop (exactly)?
microbenchmark(for (i in 1:100) {i + 5})
```


## Measuring performance: memory

What part of a program (or which object) takes up how much memory? 

A simple example:
```{r}
hello <- "Hello, World!"
object.size(hello)
```

## Measuring performance: changes in memory


```{r}
# load package
library(pryr)

# initiate a vector with 1000 (pseudo)-random numbers
mem_change(
        thousand_numbers <- runif(1000)
        )

```

```{r}

# initiate a vector with 1M (pseudo)-random numbers
mem_change(
        a_million_numbers <- runif(1000^2)
        )
```

## Profiling

Assess several lines of code/an entire script. Get overview of timing and memory usage. Find bottlenecks. 

- `bench` package: overview in console, compare different implementations.
- `profvis` package: graphical overview.

## `bench::mark()` example

*example in R* (see `lecture2.R`)


## `profvis` example

*example in R* (see `lecture2.R`)


## Overview/summary 

package | function | purpose
-------- | ---------- | ---------------------------------------------
`utils`  | `object.size()` | Provides an estimate of the memory that is being used to store an R object.
`pryr`   | `object_size()` | Works similarly to `object.size()`, but counts more accurately and includes the size of environments.
`pryr` | `mem_used()`     | Returns the total amount of memory (in megabytes) currently used by R.
`pryr` | `mem_change()`   | Shows the change in memory (in megabytes) before and after running code.
`base`   | `system.time()` | Returns CPU (and other) times that an R expression used.
`microbenchmark` | `microbenchmark()` | Highly accurate timing of R expression evaluation.
`bench` |  `mark()` | Benchmark a series of functions.
`profvis`| `profvis()`   | Profiles an R expression and visualizes the profiling data (usage of memory, time elapsed, etc.).



# Writing efficient R code

## Memory allocation and growing objects

- R tends to *"grow"* already initiated *objects in memory* when they are modified.
- Initially, a small amount of memory is occupied by the object at some location in memory.
- *Problem*: growing objects mean re-allocating memory, which needs time!


## Example to illustrate the problem: for-loops 

Context: implement a function that computes the square-root of each element value in a numeric vector.
Two approaches: a) naïve implementation (ignorant of growing objects), b) pre-allocation of memory.

```{r}
# a) naïve implementation
sqrt_vector <- 
     function(x) {
          output <- c()
          for (i in 1:length(x)) {
               output <- c(output, x[i]^(1/2))
          }
          
          return(output)
     }

```

## Example to illustrate the problem: for-loops 

```{r}
# b) implementation with pre-allocation of memory
sqrt_vector_faster <- 
     function(x) {
          output <- rep(NA, length(x))
          for (i in 1:length(x)) {
               output[i] <-  x[i]^(1/2)
          }
          
          return(output)
     }
```



## Example to illustrate the problem: for-loops 

```{r}
# the different sizes of the vectors we will put into the two functions
input_sizes <- seq(from = 100, to = 10000, by = 100)
# create the input vectors
inputs <- sapply(input_sizes, rnorm)

# compute outputs for each of the functions
output_slower <- 
     sapply(inputs, 
            function(x){ system.time(sqrt_vector(x))["elapsed"]
                 }
            )
output_faster <- 
     sapply(inputs, 
            function(x){ system.time(sqrt_vector_faster(x))["elapsed"]
                 }
            )
```

## Example to illustrate the problem: for-loops 

```{r}
# load packages
library(ggplot2)

# initiate data frame for plot
plotdata <- data.frame(time_elapsed = c(output_slower, output_faster),
                       input_size = c(input_sizes, input_sizes),
                       Implementation= c(rep("sqrt_vector", length(output_slower)),
                            rep("sqrt_vector_faster", length(output_faster))))

# plot
ggplot(plotdata, aes(x=input_size, y= time_elapsed)) +
     geom_point(aes(colour=Implementation)) +
     theme_minimal(base_size = 18) +
     theme(legend.position = "bottom") +
     ylab("Time elapsed (in seconds)") +
     xlab("No. of elements processed") 
     
```

## Memory allocation and growing objects: take-away message

- *Avoid growing objects, pre-allocate memory!*
- Simple (but important) case: properly initiate container for *for-loop* results. 


## Vectorization in basic R functions

- In R 'everything is a vector' and many of the most basic R functions (such as math operators) are *vectorized*.  
- Directly work on vectors, take advantage of the similarity of each of the vector's elements.
- Contrast: In a simple loop, R has to go through the same 'preparatory' steps again and again in each iteration.


## Vectorization in basic R functions: illustration

```{r}
# implementation with vectorization
sqrt_vector_fastest <- 
     function(x) {
               output <-  x^(1/2)
          return(output)
     }

# speed test
output_fastest <- 
     sapply(inputs, 
            function(x){ system.time(sqrt_vector_fastest(x))["elapsed"]
                 }
            )
```


## Vectorization in basic R functions: illustration

```{r}
# load packages
library(ggplot2)

# initiate data frame for plot
plotdata <- data.frame(time_elapsed = c(output_faster, output_fastest),
                       input_size = c(input_sizes, input_sizes),
                       Implementation= c(rep("sqrt_vector_faster", length(output_faster)),
                            rep("sqrt_vector_fastest", length(output_fastest))))

# plot
ggplot(plotdata, aes(x=time_elapsed, y=Implementation)) +
     geom_boxplot(aes(colour=Implementation),
                          show.legend = FALSE) +
     theme_minimal(base_size = 18) +
     xlab("Time elapsed (in seconds)")
```

## Vectorization: take-away message

- *Make use of vectorization (in basic R functions)*
- Simple (but important) case: math operators.


## `apply`-type functions in R

Looks like vectorization, but actually runs a loop under the hood.

Example: use `sapply` to compute column-wise averages.

```{r}
# load and inspect example data (see ?midwest for details)
data("midwest")
head(midwest)
# compute column averages for columns 5 to 11
sapply(midwest[, 5:11], mean)

```

## `apply`-type functions in R:  take-away message

- Instead of writing simple loops, use `apply`-type functions to save time writing code (and make the code easier to read).
- This automatically avoids the memory-allocation problems with growing objects.
- `apply`-type functions look like vectorization, but are actually running loops.


## Avoid unnecessary copying

- Objects/values do not have names but _names have values_!
- Objects have a 'memory address'/identifiers.
- We can 'bind' several different names to values.

## Illustration: binding


```{r}
# initiate object
a <- runif(10000)
# link other name to values
b <- a

# proof that this is not copying
object_size(a)
mem_change(c <- a)
```

## Illustration: memory address

```{r message=FALSE, warning=FALSE}
# load packages
library(lobstr)

# check memory addresses of objects
obj_addr(a)
obj_addr(b)
```

## Illustration: copy-on-modify

```{r}
# check the first element's value
a[1]
b[1]

# modify a, check memory change
mem_change(a[1] <- 0)

# check memory addresses
obj_addr(a)
obj_addr(b)

```
The entire vector was copied!

## Illustration: modify in place

With a single binding, no need to copy!

```{r}
mem_change(d <- runif(10000))
mem_change(d[1] <- 0)
```


## Avoid unnecessary copying: take-away message

- In practice (more complex code) it is often hard to predict whether or not a copy will occur.
  - E.g., usual R functions vs. 'primitive' C functions.
- Use `tracemem()` to check your code for potential improvements (avoid unnecessary copying).


## Releasing memory

- If your program uses up a lot of memory, all processes on your computer might substantially slow down. 
- Remove/delete an object once you do not need it anymore with the `rm()` function.

Example: 

```{r}
mem_change(large_vector <- runif(10^8))
mem_change(rm(large_vector))
```

## Releasing memory: take-away message
- `rm()` removes objects that are currently accessible in the global R environment.
- However, some objects/values might technically not be visible/accessible anymore.
- Solution: call `gc()` (the garbage collector).
- R will automatically run the garbage collector once it is close to running out of memory, but, explicitly calling `gc` can still improve the performance of your script when working with large data sets


## Beyond R

- For advanced programmers, R offers various options to directly make use of compiled programs (for example, written in C, C++, or FORTRAN). 
- Several of the core R functions are implemented in one of these lower-level programming languages.

## Beyond R

```{r}
# inspect source code
sum
```

`.Primitive()` indicates that `sum()` is actually referring to an internal function (in this case implemented in C).


# SQL basics

## SQL basics

> Are tomorrow’s bigger computers going to solve the problem? For some people,
yes—their data will stay the same size and computers will get big enough to
hold it comfortably. For other people it will only get worse—more powerful
computers means extraordinarily larger datasets. If you are likely to be in this
latter group, you might want to get used to working with databases now. 

[@burns_2011]

## Structured Query Language

- Traditionally only encountered in the context of *relational database management systems*.
- Now also used to query data from *data warehouse* systems (e.g. Amazon Redshift) and even to query massive amounts (terabytes or even petabytes) of data stored in *data lakes* (e.g., Amazon Athena).
- Bread-and-butter tool to query structured data.

## What is it for? Illustration

Query/prepare/join data for analysis. Let's create a point of reference in R.


## What is it for? Illustration
```{r echo=FALSE, message=FALSE, purl=FALSE}
library(DBI)
library(sqldf)
con <- dbConnect(RSQLite::SQLite(), "../data/mydb.sqlite")
econ <- read.csv("../data/economics.csv")
dbWriteTable(con, "econ", econ, field.types=c("date"="DATE", "pop"="REAL"), overwrite=TRUE )
```

In SQL, we can get exactly the same, with the following command.

```{sql connection=con}
SELECT 
strftime('%Y', `date`)  AS year,
AVG(unemploy) AS average_unemploy
FROM econ
WHERE "1968-01-01"<=`date`
GROUP BY year LIMIT 6;

```

Key take-away: in R, we instruct the computer what to do in order to get the table we want. In SQL we instruct the computer what table we want.


<!-- The Structured Query Language (SQL) has become a bread-and-butter tool for data analysts and data scientists due to its broad application in systems used to store large amounts of data. While traditionally only encountered in the context of structured data stored in relational database management systems, some versions of it are now also used to query data from data warehouse systems (e.g. Amazon Redshift) and even to query massive amounts (terabytes or even petabytes) of data stored in data lakes (e.g., Amazon Athena). In all of these applications, SQL's purpose (from the data analytics' perspective) is to provide a convenient and efficient way to query data from mass storage for analysis. Instead of importing a CSV file into R and then filtering it in order to get to the analytic data set, we use SQL to express how the analytic data set should look like (which variables and rows should be included). -->


## Getting started with SQLite

- [SQLite](https://sqlite.org/index.html)
     - Free, full-featured SQL database engine.
     - Widely used across platforms.
     - Typically pre-installed on Windows/MacOSX.

## Set up an SQLite database

In this first code example, we set up an SQLite database using the command line. Open the terminal and switch to the `data` directory.

```{bash eval=FALSE, purl=FALSE}
cd data 
```

## Set up an SQLite database: initiate database


Start up SQLite, create a new (empty) database called `mydb.sqlite` and connect to the newly created database.

```{bash eval= FALSE, purl=FALSE}
sqlite3 mydb.sqlite
```

## Set up an SQLite database: create table

We create a new table called `econ` based on the same data used in the R example above.

```{sql connection=con, eval = FALSE, purl=FALSE}
CREATE TABLE econ(
"date" DATE,
"pce" REAL,
"pop" REAL,
"psavert" REAL,
"uempmed" REAL,
"unemploy" INTEGER
);

```

## Set up an SQLite database: import data to table


```{sql connection=con, eval = FALSE, purl=FALSE}
-- prepare import
.mode csv
-- import data from csv
.import --skip 1 economics.csv econ

```

## Set up an SQLite database: check db tables

Now we can have a look at the new database table in SQLite. `.tables` shows that we now have one table called `econ` in our database and `.schema` displays the structure of the new `econ` table. 

```{ purl=FALSE}
.tables
```

```{ purl=FALSE}
# econ
```


```{ purl=FALSE}
.schema econ
```

```{ purl=FALSE}
# CREATE TABLE econ(
# "date" DATE,
# "pce" REAL,
# "pop" REAL,
# "psavert" REAL,
# "uempmed" REAL,
# "unemploy" INTEGER
# );
```


## Simple queries

Set output mode
```{sql connection=con, eval = FALSE, purl=FALSE}
.header on
```

```{sql connection=con, eval = FALSE, purl=FALSE}
.mode columns
```

Select all (`*`) variable values of the observation of January 1968.

```{sql connection=con, purl=FALSE}
select * from econ where date = '1968-01-01';
```


## Simple queries

Select all dates and unemployment values of observations with more than 15 million unemployed, ordered by date.

```{sql connection=con, purl=FALSE}
select date, 
unemploy from econ 
where unemploy > 15000
order by date;
```


## Joins

Let's extend the previous example by importing an additional table to our `mydb.sqlite`. The additional data is stored in the file `inflation.csv` and contains information on the US yearly inflation rate measured in percent.

```{r echo=FALSE, message=FALSE, purl=FALSE}
library(DBI)
library(sqldf)
inflation <- read.csv("../data/inflation.csv")
dbWriteTable(con, "inflation", inflation, field.types=c("date"="DATE"), overwrite=TRUE )
```

```{sql connection=con, eval = FALSE, purl=FALSE}
-- Create the new table
CREATE TABLE inflation(
"date" DATE,
"inflation_percent" REAL
);

-- prepare import
.mode csv
-- import data from csv
.import --skip 1 inflation.csv inflation
-- switch back to column mode 
.mode columns

```


## Joins: example

- Aim: get a table that serves as basis for a [Phillips curve](https://en.wikipedia.org/wiki/Phillips_curve) plot, with yearly observations and the variables `year`, `average_unemp_percent`, and `inflation_percent`. 
- `econ` contains monthly observations, while `inflation` contains yearly observations. 
- We can combine the two data sets at the level of years. 

## Joins: R as a reference point

```{r}
# import data
econ <- read.csv("../data/economics.csv")
inflation <- read.csv("../data/inflation.csv")

# prepare variable to match observations
econ$year <- lubridate::year(econ$date)
inflation$year <- lubridate::year(inflation$date)

# create final output
years <- unique(econ$year)
averages <- sapply(years, FUN = function(x) {
        mean(econ[econ$year==x,"unemploy"]/econ[econ$year==x,"pop"])*100
        
} )
unemp <- data.frame(year=years,
                     average_unemp_percent=averages)

# combine via the year column
# keep all rows of econ
output<- merge(unemp, inflation[, c("year", "inflation_percent")], by="year")


# inspect output
head(output)

```


## The same table can be created in SQLite.

```{sql connection=con, max.print=6}
SELECT 
strftime('%Y', econ.date)  AS year,
AVG(unemploy/pop)*100 AS average_unemp_percent,
inflation_percent
FROM econ INNER JOIN inflation ON year = strftime('%Y', inflation.date)
GROUP BY year
```


```{r include=FALSE}
dbDisconnect(con)
```


## Exit SQLite

When done working with the database, we can exit SQLite with the `.quit` command.




## References {.smaller}

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):after {
  content: '';
}
</style>



