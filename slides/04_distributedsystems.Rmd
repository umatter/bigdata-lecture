---
title: "Big Data Analytics"
subtitle: "Lecture 4:<br>Hardware: Distributed Systems"
author: "Prof. Dr. Ulrich Matter"
output:
  ioslides_presentation:
    css: ../style/ioslides_unilu.css
    template: ../style/nologo_template.html
  beamer_presentation: default
logo: ../img/logo_unilu2.png
bibliography: ../references/bigdata.bib
---


```{r set-options, echo=FALSE, cache=FALSE, purl=FALSE}
options(width = 100)
library(knitr)
```


# Updates


## Goals for this session

1. Understand what a *distributed system* (basically) is and what it is used for in Big Data Analytics.
2. Know the very basics of what *MapReduce* and *Hadoop* is.
3. Know what *Spark* is. 
4. R/Application: Be familiar with how to use *Spark* via R and SQL.


# Distributed Systems

## Distributed Systems

```{r distributedsystems, echo=FALSE, out.width = "90%", fig.align='center', purl=FALSE}
include_graphics("../img/distributed_system.jpg")
```


## Distributed Systems

- The data set is literally *split up* into pieces that then reside separately on *different nodes* (different computers).
- Requires an *additional "layer" of software*: coordinates distribution/loading of data, simultaneous processing.
- Requires a *different "programming model"* to define computing/data analytics tasks. 


# MapReduce

## MapReduce

- Most broadly used *programming model* for big data processing on distributed systems.
- Two parts: 
  1. Map function sorts/filters the data (on each node). 
  2. Reduce function aggregates the sorted/filtered data. 


## MapReduce illustration: word count


<center>
Text in book 1:

*Apple Orange Mango*
&nbsp;

*Orange Grapes Plum*
&nbsp;

</center> 


<center>
Text in book 2:

*Apple Plum Mango*
&nbsp;

*Apple Apple Plum*
&nbsp;

</center> 


## MapReduce illustration: word count

```{r mapreduce, echo=FALSE, out.width = "99%", fig.align='center', fig.cap="Illustration of the MapReduce programming model.", purl=FALSE}
include_graphics("../img/mapreduce_wordcount.jpg")
```






# Hadoop

## Hadoop


```{r hadoop, echo=FALSE, out.width = "90%", fig.align='center', purl=FALSE}
include_graphics("../img/hadoop.png")
```



## Hadoop word count example


In a first step, we create an input directory where we store the input file(s) to feed to Hadoop.

```{bash eval=FALSE}
# create directory for input files (typically text files)
mkdir ~/input
```

## Hadoop word count example

Then, we add a text file containing the same text as in the example above.

```{bash eval=FALSE}
echo "Apple Orange Mango
Orange Grapes Plum
Apple Plum Mango
Apple Apple Plum" >>  ~/input/text.txt

```

## Hadoop word count example

```{bash eval=FALSE}
# run mapreduce word count
/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.1.jar wordcount ~/input ~/wc_example
```



## Hadoop word count example: see result

```{bash }
cat ~/wc_example/*
```


## Distributed Systems/MapReduce/Hadoop: take-away messages

- *Distributed system:* a cluster of computers with separate storage/memory/cpu in each node.
  - Useful for really large amounts of data (TBs and more)
- *MapReduce:* concept/programming model with which we can run analytics tasks on a cluster.
  - Split data, sort, aggregate, collect/return.
- *Hadoop:* widely used implementation of MapReduce. 


# Spark

## Spark basics

- Cluster computing platform made for data analytics.
- Based on Hadoop, but much faster in many core data analytics tasks.
- Easy to use from R.


##  Spark basics

```{r sparkstack, echo=FALSE, out.width = "60%", fig.align='center',  purl=FALSE}
include_graphics("../img/spark_components.jpg")
```

## Spark basics: RDDs

- Fundamental data structure: *resilient distributed dataset' (RDD)*.
- Distributed collections of elements.
- Manipulations are executed in parallel in these RDDs. 

## Spark in R 

- Two prominent packages connect R to Spark: `SparkR` and RStudio's `sparklyr`.
- Similarly easy to use and cover all the basics for analytics tasks.
- See, e.g., [this blog post](https://cosminsanda.com/posts/a-compelling-case-for-sparkr/) for pros and cons.



## Spark/`SparkR` installation 

- `SparkR` depends on Java (version 8). Make sure the right Java version is installed.

<!-- <!-- in Mac OS (after doing this: https://stackoverflow.com/questions/21964709/how-to-set-or-change-the-default-java-jdk-version-on-os-x) -->
<!-- ```{bash eval = FALSE} -->
<!-- cd  -->
<!-- source .bash_profile -->
<!-- j8 -->
<!-- ``` -->

<!-- ```{r} -->
<!-- system("cd -->
<!--        source .bash_profile -->
<!--        j8") -->

<!-- ``` -->


```{bash echo=FALSE, eval=FALSE}
# might have to switch to java version 8 first
sudo update-alternatives --config java 
```

## Spark/`SparkR` installation 

- Install `SparkR` from GitHub (needs the `devtools`-package): `SparkR::install.spark()`.
- After installing, run `SparkR::install.spark()` (will download and install Apache Spark)
- (Alternatively: install the `sparklyr`-package and then run `sparklyr::spark_install()`)

## Initiate an interactive `SparkR`-session 

In the terminal:

```{bash, eval=FALSE}
$ SPARK-HOME/bin/sparkR
```

where `SPARK-HOME` is a placeholder for the path to your local Spark installation (printed to the console after running `SparkR::install.spark()`).


## Run `SparkR` from within RStudio 

```{r warning=FALSE, message=FALSE, eval=FALSE}
# to install use
# devtools::install_github("cran/SparkR")

# load packages
library(SparkR)

# start session
sparkR.session()

```



```{r echo=FALSE, include=FALSE}
# install.packages("SparkR")
# or, if temporarily not available on CRAN:
#if (!require('devtools')) install.packages('devtools')
#devtools::install_github('apache/spark@v2.x.x', subdir='R/pkg') # replace x.x with the version of your spark installation

# load packages
library(SparkR)

# start session
sparkR.session(sparkHome = "/home/umatter/.cache/spark/spark-3.1.2-bin-hadoop2.7")

```






## `SparkR`: Data import and summary statistics


```{r}

# Import data and create a SparkDataFrame (a distributed collection of data, RDD)
flights <- read.df("../data/flights.csv", source = "csv", header="true")


# inspect the object
class(flights)
head(flights)

```

## `SparkR`: Set data types


```{r}
flights$dep_delay <- cast(flights$dep_delay, "double")
flights$dep_time <- cast(flights$dep_time, "double")
flights$arr_time <- cast(flights$arr_time, "double")
flights$arr_delay <- cast(flights$arr_delay, "double")
flights$air_time <- cast(flights$air_time, "double")
flights$distance <- cast(flights$distance, "double")
```


## `SparkR`: filter/select data

Variable selection and filtering of observations is implemented in `select()` and `filter()` (as in the `dplyr` package). 

```{r}
# filter
long_flights <- select(flights, "carrier", "year", "arr_delay", "distance")
long_flights <- filter(long_flights, long_flights$distance >= 1000)
head(long_flights)
```

## `SparkR`: aggregation

```{r}
# aggregation: mean delay per carrier
long_flights_delays<- summarize(groupBy(long_flights, long_flights$carrier),
                      avg_delay = mean(long_flights$arr_delay))
head(long_flights_delays)
```

## `SparkR`: fetch result as data.frame

```{r}
# Convert result back into native R object
delays <- collect(long_flights_delays)
class(delays)
delays
```


## Spark with SQL

- Directly interact with Spark via SQL!

Open a terminal window, switch to the `SPARK-HOME` directory,

```{bash eval=FALSE}
cd SPARK-HOME
```

and enter the following command.

```{bash, eval=FALSE}

$ bin/spark-sql
```

(`SPARK-HOME` is again the placeholder for the path to your local Spark installation).


## Spark with SQL: no database needed!

- Queries can directly run on data files.
- See example data sets located at `SPARK-HOME/examples/src/main/resources`.
- Can be CSV or JSON.

## Spark with SQL: JSON example

```{json}
{"name":"Michael", "salary":3000}
{"name":"Andy", "salary":4500}
{"name":"Justin", "salary":3500}
{"name":"Berta", "salary":4000}
```

## Spark with SQL: JSON example

- Query the data directly via SQL commands by referring to the location of the JSON file. 
- Example: *select all observations*

```{sql eval=FALSE}

SELECT * 
FROM json.`examples/src/main/resources/employees.json`
;

```

```
Michael 3000
Andy    4500
Justin  3500
Berta   4000
Time taken: 0.099 seconds, Fetched 4 row(s)
```

## Spark with SQL: JSON example


- Example: *filter observations*

```{sql eval=FALSE}

SELECT * 
FROM json.`examples/src/main/resources/employees.json`
WHERE salary <4000
;

```

```
Michael 3000
Justin  3500
Time taken: 0.125 seconds, Fetched 2 row(s)
```

## Spark with SQL: JSON example

- Example: *compute the average salary*

```{sql eval=FALSE}

SELECT AVG(salary) AS mean_salary 
FROM json.`examples/src/main/resources/employees.json`

;

```

```
3750.0
Time taken: 0.142 seconds, Fetched 1 row(s)
```


## Spark with R + SQL

- Combine the SQL query features of Spark and SQL with running R on Spark!
- All from within RStudio.

## Spark with R + SQL: start session, read data


```{r warning=FALSE, message=FALSE, eval=TRUE}
# to install use
# devtools::install_github("cran/SparkR")

# load packages
library(SparkR)

# start session
sparkR.session()

# read data 
flights <- read.df("../data/flights.csv", source = "csv", header="true")

```

## Make Spark data frame accessible for SQL

1. Register the Spark data frame as a temporary table/view with `createOrReplaceTempView()`
2. Run SQL queries on it from within the R session via the `sql()`-function. 
3. `sql()` will return the results as Spark data frame (this means the result is also located on the cluster and does hardly affect the master node's memory). 

```{r}
# register the data frame as a table
createOrReplaceTempView(flights, "flights" )

# now run SQL queries on it
long_flights2 <- sql("SELECT DISTINCT carrier,
                             year, 
                             arr_delay,
                             distance
                            FROM flights 
                            WHERE 1000 <= distance")
head(long_flights2)

```



## Spark (with R and SQL): take-away messages

- Based on Hadoop, but easier to use and faster.
- Accessible via R and/or SQL.
- Runs on local machine: work in RStudio, test code, then deploy.

# Q&A

## References {.smaller}

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):after {
  content: '';
}
</style>
