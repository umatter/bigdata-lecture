---
title: "Big Data Analytics"
subtitle: "Data Collection and Data Storage"
author: "Prof. Dr. Ulrich Matter"
output:
  ioslides_presentation:
    css: ../style/ioslides_unilu.css
    template: ../style/nologo_template.html
  beamer_presentation: default
logo: ../img/logo_unilu2.png
bibliography: ../references/bigdata.bib
---


```{r set-options, echo=FALSE, cache=FALSE, purl=FALSE}
options(width = 100)
library(knitr)
```


# Updates

## Goals this session

- Get to know a simple workflow for data collection
- Understand the basics of RDBMS. Why "efficient"? Useful for which situations?
- Review: set up and run SQLite. 
- R skills: Connect SQLite with R (locally).
- Cloud computing: Know how to set up a MySQL DB on AWS RDS.
- Know the very basics of data warehouses (example: BigQuery) and data lakes (example: AWS S3)

# Data Pipeline

## Data Pipeline  {data-background=#ffffff}

```{r datapipeline, echo=FALSE, out.width = "90%", fig.align='center',  purl=FALSE}
include_graphics("../img/data_pipeline.png")
```


# Data Collection and Data Storage

## Gathering and compilation of raw data

### NYC taxi data

- The raw data consists of several monthly CSV-files and can be downloaded via the [TLC's website](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). 

- The following short R-script automates the downloading of all available trip-record files. *NOTE*: Downloading all files can take several hours and will occupy over 200GB!

```{r eval=FALSE}
# Fetch all TLC trip records
# Data source: 
# https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page
# Input: Monthly Parquet files from urls

# SET UP -----------------

# packages
library(R.utils) # to create directories from within R

# fix vars
BASE_URL <- "https://d37ci6vzurychx.cloudfront.net/trip-data/"
FILE <- "yellow_tripdata_2018-01.parquet"
URL <- paste0(BASE_URL, FILE)
OUTPUT_PATH <- "data/tlc_trips/"
START_DATE <- as.Date("2009-01-01")
END_DATE <- as.Date("2018-06-01")


# BUILD URLS -----------

# parse base url
base_url <- gsub("2018-01.parquet", "", URL)
# build urls
dates <- seq(from= START_DATE,
                   to = END_DATE,
                   by = "month")
year_months <- gsub("-01$", "", as.character(dates))
data_urls <- paste0(base_url, year_months, ".parquet")
data_paths <- paste0(OUTPUT_PATH, year_months, ".parquet")

# FETCH AND STACK CSVS ----------------

mkdirs(OUTPUT_PATH)
# download all csvs in the data range
for (i in 1:length(data_urls)) {
     
     # download to disk
     download.file(data_urls[i], data_paths[i])
}


```



## Stack/combine raw source files

- **Aim**: have one CSV file with all the data.
- **Approach**: combine the downloaded Parquet files into one compressed CSV file.
- We do this with the `arrow` package, an R interface to **Apache Arrow** (a platform to work with large-scale columnar data).

## Stack/combine raw source files

We start by installing the `arrow` package\index{arrow package}  in the following way.

```{r eval=FALSE}
# install arrow
Sys.setenv(LIBARROW_MINIMAL = "false") # to enable working with compressed files
install.packages("arrow") # might take a while

```

## Stack/combine raw source files


```{r message=FALSE, warning=FALSE}

# SET UP ---------------------------

# load packages
library(arrow)
library(data.table)
library(purrr)

# fix vars
INPUT_PATH <- "../data/tlc_trips/"
OUTPUT_FILE <- "../data/tlc_trips.parquet"
OUTPUT_FILE_CSV <- "../data/tlc_trips.csv"

# list of paths to downloaded Parquet files
all_files <- list.files(INPUT_PATH, full.names = TRUE)

# LOAD, COMBINE, STORE ----------------------

# read Parquet files
all_data <- lapply(all_files, read_parquet, as_data_frame = FALSE)

# combine all arrow tables into one
combined_data <- lift_dl(concat_tables)(all_data)

# write combined dataset to csv file
write_csv_arrow(combined_data,
                file = OUTPUT_FILE_CSV, 
                include_header = TRUE)

```


# Data Storage and Databases


## (Big) Data Storage

 - $(I)$ How can we store large data sets permanently on a mass storage device in an efficient way (here, efficient can be understood as 'not taking up too much space')?
 - $(II)$ How can we load (parts of) this data set in an efficient way (here, efficient~fast) for analysis?

## We look at this problem in two situations: 

 - The data need to be stored locally (e.g., on the hard disk of our laptop).
 - The data can be stored on a server in the cloud.
 
## We look at three types of systems: 

 - Relational databases
 - Data warehouse solutions
 - Data lake/simple storage solutions

## Many new database types for Big Data

```{r whatis, echo=FALSE, out.width = "80%", fig.align='center', purl=FALSE, fig.cap="NoSQL/NewSQL systems. Source: https://img.deusm.com/informationweek/2014/06/1269559/NoSQL-&-NewSQL.jpg"}
include_graphics("https://img.deusm.com/informationweek/2014/06/1269559/NoSQL-&-NewSQL.jpg")
```




## Simple distinction

- *SQL/Relational Database Systems (RDBMS)*: Relational data model, tabular relations.
     - In use for a long time, very mature, very accurate/stable.
- *NoSQL ('non-SQL', sometimes 'Not only SQL')*: Different data models, column, document, key-value, graph.
     - Horizontal scaling.
     - Non-tabular data.
     - Typically used to handle very large amounts of data.

## Row-based vs Column-based

- *Row-based*: SQL databases (e.g., SQLite)
     - Changing one value, means updating a row.
     - Efficient when users often access many columns and rather few observations.
- *Column-based*: Some data warehouse and data lake systems, e.g., Google BigQuery
     - Efficient when users from time to time query few columns but vast amounts of observations.


## RDBMS basics

- *Relational data model*
     - Data split into several tables (avoid redundancies).
     - Tables are linked via key-variables/columns.
     - Save storage space.
- *Indexing*
     - Table columns (particularly keys) are indexed.
     - Reduces number of disk accesses required to query data.
     - Makes querying/loading of data more efficient/faster.




## Getting started with (R)SQLite

- [SQLite](https://sqlite.org/index.html)
     - Free, full-featured SQL database engine.
     - Widely used across platforms.
     - Typically pre-installed on Windows/MacOSX.
- [RSQLite](https://cran.r-project.org/web/packages/RSQLite/index.html)
     - Embeds SQLite in R.
     - Use SQLite from within an R session.


## Exercise 1:  First steps in SQLite (Terminal)

- Set up a new database called `mydb.sqlite`.

```{bash eval=FALSE}
cd materials/data 
```

```{bash eval= FALSE}
sqlite3 mydb.sqlite
```

```{sql eval = FALSE}
.tables
```


## Import data from CSV files

```{r echo=FALSE, message=FALSE}
library(DBI)
con <- dbConnect(RSQLite::SQLite(), "../data/mydb.sqlite")
```


```{sql connection=con, eval = FALSE}
CREATE TABLE econ(
"date" DATE,
"pce" REAL,
"pop" INTEGER,
"psavert" REAL,
"uempmed" REAL,
"unemploy" INTEGER
);

.mode csv
.import economics.csv econ
```


## Inspect the database


```{}
.tables
```

```{}
# econ
```

```{}
.schema econ
```

```{}
# CREATE TABLE econ(
# "date" DATE,
# "pce" REAL,
# "pop" INTEGER,
# "psavert" REAL,
# "uempmed" REAL,
# "unemploy" INTEGER
# );
```

## Set options for output

```{sql connection=con, eval = FALSE}
.header on
```

```{sql connection=con, eval = FALSE}
.mode columns
```



## Issue queries: Example 1

In our first query, we select all (`*`) variable values of the observation of January 1968.

```{sql connection=con}
select * from econ where date = '1968-01-01'
```

## Issue queries: Example 2

Now let's select all year/months in which there were more than 15 million unemployed, ordered by date.

```{sql connection=con}
select date from econ 
where unemploy > 15000
order by date;
```

## Close SQLite

When done working with the database, we can exit SQLite with the `.quit` command.




## Exercise 2: Indices and joins

 - Import several related tables.
 - Add indices to tables. 

## Initiate DB, import data

We set up a new database called `air.sqlite` and import the csv-file `flights.csv` (used in previous lectures) as a first table.

```{bash echo=TRUE, eval=FALSE}
# create database and run sqlite
sqlite3 air.sqlite

```

## Import data from CSVs


```{sql connection=con, eval = FALSE}
.mode csv
.import flights.csv flights
```


```{r echo=FALSE, message=FALSE}
library(DBI)
# set up a connection for the examples
con_air <- dbConnect(RSQLite::SQLite(), "../data/air_final.sqlite")
```


## Inspect the `flights` table

Again, we can check if everything worked out well with `.tables` and `.schema`.


```{sql connection=con_air, eval = FALSE}
.tables
.schema flights
```

## Related tables 

- [`airports.csv`](http://stat-computing.org/dataexpo/2009/airports.csv): Describes the locations of US Airports (relates to `origin` and `dest`).
- [`carriers.csv`](http://stat-computing.org/dataexpo/2009/carriers.csv): A listing of carrier codes with full names (relates to the `carrier`-column in `flights`.


```{r echo=FALSE, eval=FALSE}
# ASA source
URL_AIRPORTS <- "http://stat-computing.org/dataexpo/2009/airports.csv"
URL_CARRIERS <- "http://stat-computing.org/dataexpo/2009/carriers.csv"

# download
download.file(URL_AIRPORTS, destfile = "../data/airports.csv", quiet = TRUE)
download.file(URL_CARRIERS, destfile = "../data/carriers.csv", quiet = TRUE)

# re-format (facilitates import)
fwrite(fread("../data/airports.csv"), "../data/airports.csv")
fwrite(fread("../data/carriers.csv"), "../data/carriers.csv")

```


## Import related tables

Import from csv-file
```{sql connection=con_air, eval = FALSE}
.mode csv
.import airports.csv airports
.import carriers.csv carriers
```

Inspect the result
```{sql connection=con_air, eval = FALSE}
.tables
.schema airports
.schema carriers
```

## Issue queries with joins

 - Goal: A table containing flights data for all `United Air Lines Inc.`-flights departing from `Newark Intl` airport, ordered by flight number. 
 - For the sake of the exercise, we only show the first 10 results of this query (`LIMIT 10`).


## Issue queries with joins

```{sql connection=con_air, eval = TRUE}
SELECT 
year,
month, 
day,
dep_delay,
flight
FROM (flights INNER JOIN airports ON flights.origin=airports.iata) 
INNER JOIN carriers ON flights.carrier = carriers.Code
WHERE carriers.Description = 'United Air Lines Inc.'
AND airports.airport = 'Newark Intl'
ORDER BY flight
LIMIT 10;

```

## Add indices

```{sql connection=con_air, eval = FALSE}
CREATE INDEX iata_airports ON airports (iata);
CREATE INDEX origin_flights ON flights (origin);
CREATE INDEX carrier_flights ON flights (carrier);
CREATE INDEX code_carriers ON carriers (code);

```


## Re-run the query (with indices)

```{sql connection=con_air, eval = TRUE}
SELECT 
year,
month, 
day,
dep_delay,
flight
FROM (flights INNER JOIN airports ON flights.origin=airports.iata) 
INNER JOIN carriers ON flights.carrier = carriers.Code
WHERE carriers.Description = 'United Air Lines Inc.'
AND airports.airport = 'Newark Intl'
ORDER BY flight
LIMIT 10;

```



## SQLite from within R

- Use `RSQLite` to set up and query `air.sqlite` as shown above.
- All done from within an R session.


## Creating a new database with `RSQLite`

```{r}
# load packages
library(RSQLite)

# initiate the database
con_air <- dbConnect(SQLite(), "../data/air.sqlite")
```

## Importing data


```{r warning=FALSE, message=FALSE}
# load packages
library(data.table)

# import data into current R sesssion
flights <- fread("../data/flights.csv")
airports <- fread("../data/airports.csv")
carriers <- fread("../data/carriers.csv")

# add tables to database
dbWriteTable(con_air, "flights", flights)
dbWriteTable(con_air, "airports", airports)
dbWriteTable(con_air, "carriers", carriers)

```

## Issue queries with `RSQLite`

```{r}
# define query
delay_query <-
"SELECT 
year,
month, 
day,
dep_delay,
flight
FROM (flights INNER JOIN airports ON flights.origin=airports.iata) 
INNER JOIN carriers ON flights.carrier = carriers.Code
WHERE carriers.Description = 'United Air Lines Inc.'
AND airports.airport = 'Newark Intl'
ORDER BY flight
LIMIT 10;
"
```

## Issue queries with `RSQLite`

```{r}
# issue query
delays_df <- dbGetQuery(con_air, delay_query)
delays_df
```

## Close the connection to SQLite

```{r}
dbDisconnect(con_air)
```


```{r echo=FALSE}
# clean up
unlink("../data/air.sqlite")
```

# Data warehouses and BigQuery with R

## Data warehouse solutions in the cloud

- Unlike RMDBSs, main purpose is ususally analytics.
- Well organized/structured data, but not as stringent as RMDBS.
- Set up for large amounts of data.
- Flexible usage, cheap storage space, pay for data processing.
- Typically column-based.

## Google BigQuery

- Flexible, easy-to-use, hardly any set up costs.
- Pay per TB processed (only minor fees for storage etc.).
- `bigrquery`: R package to work with BigQuery via R (see https://bigrquery.r-dbi.org/)

## Get started with `bigrquery`

- Needed: Google Account.
- Go to https://cloud.google.com/bigquery.
- Click on "Try Big Query" (if new to this) or "Go to console" (if used previously).
- Create a project to use BigQuery with.
- In R: `install.packages("bigrquery")`


## A simple example: set up


```{r eval=FALSE}
# load packages, credentials
library(bigrquery)
library(data.table)
library(DBI)

# fix vars
# the project ID on BigQuery (billing must be enabled)
BILLING <- "bda-examples" 
# the project name on BigQuery
PROJECT <- "bigquery-public-data" 
DATASET <- "google_analytics_sample"

# connect to DB on BigQuery
con <- dbConnect(
     bigrquery::bigquery(),
     project = PROJECT,
     dataset = DATASET,
     billing = BILLING
)

```

## A simple example: run queries on BigQuery

```{r eval=FALSE}
# run query
query <-
"
SELECT DISTINCT trafficSource.source AS origin,
COUNT(trafficSource.source) AS no_occ
FROM `bigquery-public-data.google_analytics_sample.ga_sessions_20170801`
GROUP BY trafficSource.source
ORDER BY no_occ DESC;
"
ga <- as.data.table(dbGetQuery(con, query, page_size=15000))
head(ga)
```


# Data lakes and simple storage service

## Data lakes

- Where all your data resides in the cloud (all kind of formats/structures).
- For a data analytics project: keep at least all the raw data here (or even all the data).
- Possible basis: simple storage service in the cloud (such as *AWS S3*)
- Then use Analytics tools to run on storage (e.g., AWS Athena)

## AWS S3 with R: first steps

Prerequisites:

- AWS account
- IAM credentials (keypair) with the right to access S3
- `install.packages("aws.s3")`

## Connect R-session with S3

- `AWS_ACCESS_KEY_ID`: your access key id (of the keypair with rights to use S3)
- `AWS_SECRET_KEY`: your access key (of the keypair with rights to use S3)
- `REGION`: the region in which your S3 bucket is located (e.g., `"eu-central-1"`)


```{r eval=FALSE}
# fetch current
Sys.setenv("AWS_ACCESS_KEY_ID" = AWS_ACCESS_KEY_ID,
           "AWS_SECRET_ACCESS_KEY" = AWS_SECRET_KEY,
           "AWS_DEFAULT_REGION" = REGION)
```

## Upload files to S3

- `BUCKET`: the name of the S3 bucket to upload data to.

```{r eval=FALSE}
# upload to bucket
put_object(
  file = "data/flights.csv", # the file you want to upload
  object = "flights.csv", # the name the file will have in the S3 bucket
  bucket = BUCKET
)
```


## More than just simple storage: S3 + Amazon Athena

- **Amazon Athena**: interesting approach to combine the concept of a data lake with the concept of a data warehouse.
- Amazon Athena can directly be used to query/analyze the data stored in the simple storage service (S3). 
- We will rely on via the `RJDBC`-package to query data from S3 via Athena.

## S3 + Amazon Athena set up

```{r echo=FALSE, warning=FALSE, message=FALSE}
# load packages
library(DBI)
library(aws.s3)

# credentials and region
AWS_ACCESS_KEY_ID <- read.csv("../../BigData/_keys/umatter_accessKeys.csv")[,1]
AWS_ACCESS_KEY <- read.csv("../../BigData/_keys/umatter_accessKeys.csv")[,2]
REGION <- "eu-central-1"

```


```{r eval=FALSE}
# SET UP -------------------------

# load packages
library(DBI)
library(aws.s3)
# aws credentials with Athena and S3 rights and region
AWS_ACCESS_KEY_ID <- "YOUR_KEY_ID"
AWS_ACCESS_KEY <- "YOUR_KEY"
REGION <- "eu-central-1"

```

```{r}
# establish AWS connection
Sys.setenv("AWS_ACCESS_KEY_ID" = AWS_ACCESS_KEY_ID,
           "AWS_SECRET_ACCESS_KEY" = AWS_ACCESS_KEY,
           "AWS_DEFAULT_REGION" = REGION)
```


## S3 + Amazon Athena set up

```{r eval=FALSE}
OUTPUT_BUCKET <- "bda-athena"
put_bucket(OUTPUT_BUCKET, region="us-east-1")
```


## Query S3 with Amazon Athena

```{r message=FALSE, warning=FALSE }

# load packages
library(RJDBC)
library(DBI)

# download Athena JDBC driver
URL <- "https://s3.amazonaws.com/athena-downloads/drivers/JDBC/"
VERSION <- "AthenaJDBC_1.1.0/AthenaJDBC41-1.1.0.jar"
DRV_FILE <- "AthenaJDBC41-1.1.0.jar"
download.file(paste0(URL,VERSION), destfile = DRV_FILE)

# connect to JDBC
athena <- JDBC(driverClass="com.amazonaws.athena.jdbc.AthenaDriver", 
            DRV_FILE, 
            identifier.quote="'")
# connect to Athena
con <- dbConnect(athena, 
                 'jdbc:awsathena://athena.us-east-1.amazonaws.com:443/',
                 s3_staging_dir="s3://bda-athena",
                 user=AWS_ACCESS_KEY_ID,
                 password=AWS_ACCESS_KEY)

```

## Query S3 with Amazon Athena

Querying S3 via Athena requires the creation of an *external table* from the data stored on S3.

```{r eval=FALSE}
query_create_table <-
"
CREATE EXTERNAL TABLE default.trips (
  `vendor_name` string,
  `Trip_Pickup_DateTime` string,
  `Trip_Dropoff_DateTime` string,
  `Passenger_Count` int,
  `Trip_Distance` double,
  `Start_Lon` double,
  `Start_Lat` double,
  `Rate_Code` string,
  `store_and_forward` string,
  `End_Lon` double,
  `End_Lat` double,
  `Payment_Type` string,
  `Fare_Amt` double,
  `surcharge` double,
  `mta_tax` string,
  `Tip_Amt` double,
  `Tolls_Amt` double,
  `Total_Amt` double
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION 's3://tlc-trips/analytic_data/'
"
dbSendQuery(con, query_create_table)
```


## Query S3 with Amazon Athena

Run a test query to verify the table.

```{r}
test_query <-
"
SELECT * 
FROM default.trips
LIMIT 10
"
test <- dbGetQuery(con, test_query)
dim(test)
```

Finally, close the connection.

```{r}
dbDisconnect(con)
```



## References {.smaller}

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):after {
  content: '';
}
</style>


